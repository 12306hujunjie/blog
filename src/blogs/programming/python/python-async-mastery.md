---
title: Python å¼‚æ­¥ç¼–ç¨‹å®Œå…¨æŒ‡å—ï¼šä»åŸç†åˆ°å®æˆ˜ä¼˜åŒ–
date: 2024-01-01
categories:
  - programming
  - python
tags:
  - Python
  - å¼‚æ­¥ç¼–ç¨‹
  - asyncio
  - æ€§èƒ½ä¼˜åŒ–
  - å¹¶å‘
difficulty: advanced
readingTime: 30min
prerequisites:
  - Python åŸºç¡€è¯­æ³•
  - å‡½æ•°å’Œè£…é¥°å™¨
  - åŸºæœ¬çš„ç½‘ç»œç¼–ç¨‹æ¦‚å¿µ
learningObjectives:
  - æ·±å…¥ç†è§£ Python å¼‚æ­¥ç¼–ç¨‹åŸç†
  - æŒæ¡ asyncio æ ¸å¿ƒç»„ä»¶å’Œä½¿ç”¨æ¨¡å¼
  - å­¦ä¼šå¼‚æ­¥ç¼–ç¨‹æ€§èƒ½ä¼˜åŒ–æŠ€å·§
  - èƒ½å¤Ÿè®¾è®¡å’Œå®ç°é«˜æ€§èƒ½å¼‚æ­¥åº”ç”¨
relatedPosts:
  - python-performance-optimization
  - python-concurrency-patterns
  - python-web-frameworks
---

# Python å¼‚æ­¥ç¼–ç¨‹å®Œå…¨æŒ‡å—ï¼šä»åŸç†åˆ°å®æˆ˜ä¼˜åŒ–

## ğŸ“– å¯¼è¯»

åœ¨ç°ä»£ Web åº”ç”¨å’Œå¾®æœåŠ¡æ¶æ„ä¸­ï¼Œå¤„ç†é«˜å¹¶å‘ I/O å¯†é›†å‹ä»»åŠ¡æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚Python çš„å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹æä¾›äº†ä¸€ç§ä¼˜é›…è€Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡å°†ä»åº•å±‚åŸç†å‡ºå‘ï¼Œæ·±å…¥å‰–æ Python å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µï¼Œé€šè¿‡å®æˆ˜æ¡ˆä¾‹å±•ç¤ºå¦‚ä½•æ„å»ºé«˜æ€§èƒ½å¼‚æ­¥åº”ç”¨ï¼Œå¹¶åˆ†äº«ç”Ÿäº§ç¯å¢ƒä¸­çš„ä¼˜åŒ–ç»éªŒå’Œæœ€ä½³å®è·µã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ–‡ï¼Œä½ å°†ï¼š
- **ç†è§£äº‹ä»¶å¾ªç¯æœºåˆ¶**ï¼šæŒæ¡ asyncio çš„æ ¸å¿ƒè¿è¡ŒåŸç†
- **ç²¾é€šåç¨‹ç¼–ç¨‹**ï¼šä»åŸºç¡€è¯­æ³•åˆ°é«˜çº§æ¨¡å¼çš„å…¨é¢æŒæ¡
- **ä¼˜åŒ–å¼‚æ­¥æ€§èƒ½**ï¼šå­¦ä¼šè¯†åˆ«å’Œè§£å†³æ€§èƒ½ç“¶é¢ˆ
- **å¤„ç†å¤æ‚åœºæ™¯**ï¼šé”™è¯¯å¤„ç†ã€å¹¶å‘æ§åˆ¶ã€èµ„æºç®¡ç†
- **æ„å»ºç”Ÿäº§çº§åº”ç”¨**ï¼šçœŸå®æ¡ˆä¾‹å’Œæœ€ä½³å®è·µ

## ğŸ—ï¸ çŸ¥è¯†æ¶æ„

```mermaid
graph TB
    A[Python å¼‚æ­¥ç¼–ç¨‹] --> B[åŸºç¡€æ¦‚å¿µ]
    A --> C[æ ¸å¿ƒç»„ä»¶]
    A --> D[å®æˆ˜åº”ç”¨]
    A --> E[æ€§èƒ½ä¼˜åŒ–]
    
    B --> B1[åŒæ­¥ vs å¼‚æ­¥]
    B --> B2[å¹¶å‘ vs å¹¶è¡Œ]
    B --> B3[äº‹ä»¶é©±åŠ¨æ¨¡å‹]
    
    C --> C1[åç¨‹ Coroutine]
    C --> C2[ä»»åŠ¡ Task]
    C --> C3[äº‹ä»¶å¾ªç¯ Event Loop]
    C --> C4[Future å¯¹è±¡]
    
    D --> D1[å¼‚æ­¥ HTTP å®¢æˆ·ç«¯]
    D --> D2[å¼‚æ­¥ Web æœåŠ¡å™¨]
    D --> D3[æ•°æ®åº“è¿æ¥æ± ]
    D --> D4[æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ]
    
    E --> E1[æ€§èƒ½åˆ†æ]
    E --> E2[å¹¶å‘æ§åˆ¶]
    E --> E3[å†…å­˜ä¼˜åŒ–]
    E --> E4[é”™è¯¯å¤„ç†]
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. åŒæ­¥ vs å¼‚æ­¥ï¼šæœ¬è´¨åŒºåˆ«

```python
import time
import asyncio
import aiohttp
import requests
from typing import List, Dict

# åŒæ­¥æ–¹å¼ï¼šé˜»å¡ç­‰å¾…
def sync_fetch_url(url: str) -> Dict:
    """åŒæ­¥è·å– URL å†…å®¹"""
    start = time.time()
    response = requests.get(url)
    elapsed = time.time() - start
    return {
        'url': url,
        'status': response.status_code,
        'length': len(response.content),
        'time': elapsed
    }

def sync_fetch_multiple(urls: List[str]) -> List[Dict]:
    """åŒæ­¥è·å–å¤šä¸ª URL"""
    results = []
    total_start = time.time()
    
    for url in urls:
        result = sync_fetch_url(url)
        results.append(result)
        print(f"åŒæ­¥å®Œæˆ: {url} - {result['time']:.2f}ç§’")
    
    total_time = time.time() - total_start
    print(f"åŒæ­¥æ€»è€—æ—¶: {total_time:.2f}ç§’")
    return results

# å¼‚æ­¥æ–¹å¼ï¼šéé˜»å¡å¹¶å‘
async def async_fetch_url(session: aiohttp.ClientSession, url: str) -> Dict:
    """å¼‚æ­¥è·å– URL å†…å®¹"""
    start = time.time()
    async with session.get(url) as response:
        content = await response.read()
        elapsed = time.time() - start
        return {
            'url': url,
            'status': response.status,
            'length': len(content),
            'time': elapsed
        }

async def async_fetch_multiple(urls: List[str]) -> List[Dict]:
    """å¼‚æ­¥å¹¶å‘è·å–å¤šä¸ª URL"""
    total_start = time.time()
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for url in urls:
            task = asyncio.create_task(async_fetch_url(session, url))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        for result in results:
            print(f"å¼‚æ­¥å®Œæˆ: {result['url']} - {result['time']:.2f}ç§’")
    
    total_time = time.time() - total_start
    print(f"å¼‚æ­¥æ€»è€—æ—¶: {total_time:.2f}ç§’")
    return results

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
if __name__ == "__main__":
    urls = [
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/2",
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/3",
        "https://httpbin.org/delay/1"
    ]
    
    print("=" * 50)
    print("åŒæ­¥è¯·æ±‚æµ‹è¯•:")
    sync_results = sync_fetch_multiple(urls)
    
    print("\n" + "=" * 50)
    print("å¼‚æ­¥è¯·æ±‚æµ‹è¯•:")
    async_results = asyncio.run(async_fetch_multiple(urls))
    
    # ç»“æœåˆ†æ
    print("\n" + "=" * 50)
    print("æ€§èƒ½å¯¹æ¯”:")
    sync_total = sum(r['time'] for r in sync_results)
    async_max = max(r['time'] for r in async_results)
    print(f"åŒæ­¥ç´¯è®¡æ—¶é—´: {sync_total:.2f}ç§’")
    print(f"å¼‚æ­¥æœ€é•¿æ—¶é—´: {async_max:.2f}ç§’")
    print(f"æ€§èƒ½æå‡: {sync_total/async_max:.2f}å€")
```

### 2. äº‹ä»¶å¾ªç¯æ·±åº¦å‰–æ

```python
import asyncio
import threading
import time
from typing import Any, Callable
from dataclasses import dataclass
from enum import Enum

class TaskState(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class TaskMetrics:
    """ä»»åŠ¡åº¦é‡ä¿¡æ¯"""
    task_id: str
    state: TaskState
    start_time: float = 0
    end_time: float = 0
    error: Exception = None
    result: Any = None
    
    @property
    def duration(self) -> float:
        if self.end_time and self.start_time:
            return self.end_time - self.start_time
        return 0

class EventLoopMonitor:
    """äº‹ä»¶å¾ªç¯ç›‘æ§å™¨"""
    
    def __init__(self):
        self.tasks: Dict[str, TaskMetrics] = {}
        self.loop = None
        self._monitoring = False
        
    def start_monitoring(self, loop: asyncio.AbstractEventLoop):
        """å¼€å§‹ç›‘æ§äº‹ä»¶å¾ªç¯"""
        self.loop = loop
        self._monitoring = True
        
        # è®¾ç½®ä»»åŠ¡å·¥å‚
        loop.set_task_factory(self._task_factory)
        
        # å¯åŠ¨ç›‘æ§åç¨‹
        asyncio.create_task(self._monitor_loop())
        
    def _task_factory(self, loop, coro):
        """è‡ªå®šä¹‰ä»»åŠ¡å·¥å‚ï¼Œç”¨äºè·Ÿè¸ªä»»åŠ¡"""
        task = asyncio.Task(coro, loop=loop)
        task_id = f"task-{id(task)}"
        
        # è®°å½•ä»»åŠ¡åˆ›å»º
        self.tasks[task_id] = TaskMetrics(
            task_id=task_id,
            state=TaskState.PENDING,
            start_time=time.time()
        )
        
        # æ·»åŠ å®Œæˆå›è°ƒ
        task.add_done_callback(
            lambda t: self._on_task_done(task_id, t)
        )
        
        return task
    
    def _on_task_done(self, task_id: str, task: asyncio.Task):
        """ä»»åŠ¡å®Œæˆå›è°ƒ"""
        metrics = self.tasks.get(task_id)
        if not metrics:
            return
            
        metrics.end_time = time.time()
        
        if task.cancelled():
            metrics.state = TaskState.CANCELLED
        elif task.exception():
            metrics.state = TaskState.FAILED
            metrics.error = task.exception()
        else:
            metrics.state = TaskState.COMPLETED
            metrics.result = task.result()
    
    async def _monitor_loop(self):
        """ç›‘æ§äº‹ä»¶å¾ªç¯çŠ¶æ€"""
        while self._monitoring:
            await asyncio.sleep(1)
            
            # è·å–å½“å‰è¿è¡Œçš„ä»»åŠ¡
            current_tasks = asyncio.all_tasks(self.loop)
            
            print(f"\nğŸ“Š äº‹ä»¶å¾ªç¯çŠ¶æ€:")
            print(f"  æ´»è·ƒä»»åŠ¡æ•°: {len(current_tasks)}")
            print(f"  æ€»ä»»åŠ¡æ•°: {len(self.tasks)}")
            
            # ä»»åŠ¡çŠ¶æ€ç»Ÿè®¡
            state_counts = {}
            for metrics in self.tasks.values():
                state_counts[metrics.state] = state_counts.get(metrics.state, 0) + 1
            
            for state, count in state_counts.items():
                print(f"  {state.value}: {count}")
    
    def get_metrics(self) -> List[TaskMetrics]:
        """è·å–æ‰€æœ‰ä»»åŠ¡åº¦é‡"""
        return list(self.tasks.values())
    
    def print_summary(self):
        """æ‰“å°ä»»åŠ¡æ‰§è¡Œæ‘˜è¦"""
        print("\nğŸ“ˆ ä»»åŠ¡æ‰§è¡Œæ‘˜è¦:")
        
        completed_tasks = [
            m for m in self.tasks.values() 
            if m.state == TaskState.COMPLETED
        ]
        
        if completed_tasks:
            durations = [t.duration for t in completed_tasks]
            print(f"  å®Œæˆä»»åŠ¡æ•°: {len(completed_tasks)}")
            print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {sum(durations)/len(durations):.3f}ç§’")
            print(f"  æœ€çŸ­æ‰§è¡Œæ—¶é—´: {min(durations):.3f}ç§’")
            print(f"  æœ€é•¿æ‰§è¡Œæ—¶é—´: {max(durations):.3f}ç§’")
        
        failed_tasks = [
            m for m in self.tasks.values() 
            if m.state == TaskState.FAILED
        ]
        
        if failed_tasks:
            print(f"  å¤±è´¥ä»»åŠ¡æ•°: {len(failed_tasks)}")
            for task in failed_tasks:
                print(f"    - {task.task_id}: {task.error}")

# æ¼”ç¤ºäº‹ä»¶å¾ªç¯ç›‘æ§
async def demo_task(name: str, duration: float):
    """æ¼”ç¤ºä»»åŠ¡"""
    print(f"ğŸš€ å¯åŠ¨ä»»åŠ¡: {name}")
    await asyncio.sleep(duration)
    
    # æ¨¡æ‹Ÿéšæœºå¤±è´¥
    import random
    if random.random() < 0.2:  # 20% å¤±è´¥ç‡
        raise Exception(f"ä»»åŠ¡ {name} æ‰§è¡Œå¤±è´¥")
    
    print(f"âœ… å®Œæˆä»»åŠ¡: {name}")
    return f"Result from {name}"

async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºç›‘æ§å™¨
    monitor = EventLoopMonitor()
    loop = asyncio.get_running_loop()
    monitor.start_monitoring(loop)
    
    # åˆ›å»ºå¤šä¸ªå¹¶å‘ä»»åŠ¡
    tasks = []
    for i in range(10):
        duration = (i % 3) + 1  # 1-3 ç§’
        task = asyncio.create_task(
            demo_task(f"Task-{i}", duration)
        )
        tasks.append(task)
        await asyncio.sleep(0.1)  # ç¨å¾®é”™å¼€å¯åŠ¨æ—¶é—´
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ‰“å°ç›‘æ§æ‘˜è¦
    monitor.print_summary()
    
    # åœæ­¢ç›‘æ§
    monitor._monitoring = False

if __name__ == "__main__":
    asyncio.run(main())
```

### 3. åç¨‹çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†

```python
import asyncio
import functools
import inspect
from typing import Any, Callable, Optional
from contextlib import asynccontextmanager
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CoroutineLifecycleManager:
    """åç¨‹ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_coroutines = {}
        self.completed_coroutines = {}
        
    def track_coroutine(self, name: str = None):
        """è£…é¥°å™¨ï¼šè·Ÿè¸ªåç¨‹ç”Ÿå‘½å‘¨æœŸ"""
        def decorator(coro_func: Callable):
            @functools.wraps(coro_func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆåç¨‹ ID
                coro_name = name or coro_func.__name__
                coro_id = f"{coro_name}-{id(asyncio.current_task())}"
                
                # è®°å½•åç¨‹å¯åŠ¨
                self.active_coroutines[coro_id] = {
                    'name': coro_name,
                    'start_time': asyncio.get_event_loop().time(),
                    'task': asyncio.current_task()
                }
                
                logger.info(f"ğŸŸ¢ åç¨‹å¯åŠ¨: {coro_id}")
                
                try:
                    # æ‰§è¡Œåç¨‹
                    result = await coro_func(*args, **kwargs)
                    
                    # è®°å½•æˆåŠŸå®Œæˆ
                    self._mark_completed(coro_id, 'success', result)
                    logger.info(f"âœ… åç¨‹æˆåŠŸ: {coro_id}")
                    
                    return result
                    
                except asyncio.CancelledError:
                    # å¤„ç†å–æ¶ˆ
                    self._mark_completed(coro_id, 'cancelled', None)
                    logger.warning(f"ğŸŸ¡ åç¨‹å–æ¶ˆ: {coro_id}")
                    raise
                    
                except Exception as e:
                    # å¤„ç†å¼‚å¸¸
                    self._mark_completed(coro_id, 'failed', None, error=e)
                    logger.error(f"âŒ åç¨‹å¤±è´¥: {coro_id} - {e}")
                    raise
                    
                finally:
                    # æ¸…ç†æ´»è·ƒåç¨‹è®°å½•
                    self.active_coroutines.pop(coro_id, None)
            
            return wrapper
        return decorator
    
    def _mark_completed(self, coro_id: str, status: str, result: Any, error: Exception = None):
        """æ ‡è®°åç¨‹å®Œæˆ"""
        info = self.active_coroutines.get(coro_id, {})
        end_time = asyncio.get_event_loop().time()
        
        self.completed_coroutines[coro_id] = {
            'name': info.get('name'),
            'status': status,
            'duration': end_time - info.get('start_time', end_time),
            'result': result,
            'error': error
        }
    
    def get_active_coroutines(self):
        """è·å–æ´»è·ƒåç¨‹åˆ—è¡¨"""
        return list(self.active_coroutines.values())
    
    def get_statistics(self):
        """è·å–åç¨‹æ‰§è¡Œç»Ÿè®¡"""
        stats = {
            'active': len(self.active_coroutines),
            'completed': len(self.completed_coroutines),
            'success': 0,
            'failed': 0,
            'cancelled': 0,
            'total_duration': 0
        }
        
        for info in self.completed_coroutines.values():
            stats[info['status']] = stats.get(info['status'], 0) + 1
            stats['total_duration'] += info.get('duration', 0)
        
        if stats['completed'] > 0:
            stats['avg_duration'] = stats['total_duration'] / stats['completed']
        
        return stats

# å…¨å±€ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
lifecycle_manager = CoroutineLifecycleManager()

# åç¨‹è¶…æ—¶ç®¡ç†
class TimeoutManager:
    """åç¨‹è¶…æ—¶ç®¡ç†å™¨"""
    
    @staticmethod
    async def with_timeout(coro, timeout: float, default: Any = None):
        """ä¸ºåç¨‹æ·»åŠ è¶…æ—¶æ§åˆ¶"""
        try:
            return await asyncio.wait_for(coro, timeout)
        except asyncio.TimeoutError:
            logger.warning(f"â±ï¸ åç¨‹è¶…æ—¶ ({timeout}ç§’)")
            return default
    
    @staticmethod
    def timeout_decorator(seconds: float, default: Any = None):
        """è¶…æ—¶è£…é¥°å™¨"""
        def decorator(coro_func):
            @functools.wraps(coro_func)
            async def wrapper(*args, **kwargs):
                return await TimeoutManager.with_timeout(
                    coro_func(*args, **kwargs),
                    seconds,
                    default
                )
            return wrapper
        return decorator

# åç¨‹é‡è¯•æœºåˆ¶
class RetryManager:
    """åç¨‹é‡è¯•ç®¡ç†å™¨"""
    
    @staticmethod
    async def with_retry(
        coro_func: Callable,
        max_attempts: int = 3,
        delay: float = 1.0,
        backoff: float = 2.0,
        exceptions: tuple = (Exception,)
    ):
        """å¸¦é‡è¯•çš„åç¨‹æ‰§è¡Œ"""
        attempt = 0
        current_delay = delay
        
        while attempt < max_attempts:
            try:
                return await coro_func()
            except exceptions as e:
                attempt += 1
                if attempt >= max_attempts:
                    logger.error(f"ğŸ”„ é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§æ¬¡æ•°: {max_attempts}")
                    raise
                
                logger.warning(f"ğŸ”„ ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œç­‰å¾… {current_delay:.1f} ç§’...")
                await asyncio.sleep(current_delay)
                current_delay *= backoff
    
    @staticmethod
    def retry_decorator(
        max_attempts: int = 3,
        delay: float = 1.0,
        backoff: float = 2.0,
        exceptions: tuple = (Exception,)
    ):
        """é‡è¯•è£…é¥°å™¨"""
        def decorator(coro_func):
            @functools.wraps(coro_func)
            async def wrapper(*args, **kwargs):
                return await RetryManager.with_retry(
                    lambda: coro_func(*args, **kwargs),
                    max_attempts,
                    delay,
                    backoff,
                    exceptions
                )
            return wrapper
        return decorator

# èµ„æºç®¡ç†
@asynccontextmanager
async def managed_resource(resource_name: str):
    """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¤ºä¾‹"""
    logger.info(f"ğŸ“‚ è·å–èµ„æº: {resource_name}")
    resource = {'name': resource_name, 'data': []}
    
    try:
        yield resource
    finally:
        logger.info(f"ğŸ“ é‡Šæ”¾èµ„æº: {resource_name}")
        # æ‰§è¡Œæ¸…ç†æ“ä½œ
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ¸…ç†

# ä½¿ç”¨ç¤ºä¾‹
@lifecycle_manager.track_coroutine("æ•°æ®å¤„ç†")
@TimeoutManager.timeout_decorator(5.0, default={'error': 'timeout'})
@RetryManager.retry_decorator(max_attempts=3, delay=0.5)
async def process_data(data_id: str):
    """å¤„ç†æ•°æ®çš„åç¨‹"""
    async with managed_resource(f"data-{data_id}") as resource:
        # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
        await asyncio.sleep(1)
        
        # æ¨¡æ‹Ÿå¶å‘é”™è¯¯
        import random
        if random.random() < 0.3:
            raise ValueError(f"å¤„ç†æ•°æ® {data_id} æ—¶å‡ºé”™")
        
        resource['data'].append(f"processed-{data_id}")
        return resource

async def lifecycle_demo():
    """ç”Ÿå‘½å‘¨æœŸç®¡ç†æ¼”ç¤º"""
    tasks = []
    
    # åˆ›å»ºå¤šä¸ªæ•°æ®å¤„ç†ä»»åŠ¡
    for i in range(5):
        task = asyncio.create_task(process_data(f"item-{i}"))
        tasks.append(task)
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ‰“å°ç»“æœ
    print("\nğŸ“Š æ‰§è¡Œç»“æœ:")
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"  Task-{i}: âŒ {result}")
        else:
            print(f"  Task-{i}: âœ… {result}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = lifecycle_manager.get_statistics()
    print("\nğŸ“ˆ æ‰§è¡Œç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")

if __name__ == "__main__":
    asyncio.run(lifecycle_demo())
```

## ğŸ’» å®è·µæ¡ˆä¾‹

### 1. é«˜æ€§èƒ½å¼‚æ­¥ Web çˆ¬è™«

```python
import asyncio
import aiohttp
import aiofiles
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import Set, List, Dict, Optional
import hashlib
import json
import time
from dataclasses import dataclass, asdict
from asyncio import Semaphore
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""
    url: str
    status_code: int
    title: str
    content_length: int
    links: List[str]
    crawl_time: float
    error: Optional[str] = None

class AsyncWebCrawler:
    """é«˜æ€§èƒ½å¼‚æ­¥ç½‘ç»œçˆ¬è™«"""
    
    def __init__(
        self,
        max_concurrent: int = 10,
        max_per_host: int = 3,
        timeout: int = 10,
        max_retries: int = 3,
        user_agent: str = "AsyncCrawler/1.0"
    ):
        self.max_concurrent = max_concurrent
        self.max_per_host = max_per_host
        self.timeout = timeout
        self.max_retries = max_retries
        self.user_agent = user_agent
        
        # å¹¶å‘æ§åˆ¶
        self.semaphore = Semaphore(max_concurrent)
        self.host_semaphores: Dict[str, Semaphore] = {}
        
        # çˆ¬å–çŠ¶æ€
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()
        self.results: List[CrawlResult] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_bytes': 0,
            'start_time': 0,
            'end_time': 0
        }
    
    def _get_host_semaphore(self, url: str) -> Semaphore:
        """è·å–ä¸»æœºçº§åˆ«çš„ä¿¡å·é‡"""
        host = urlparse(url).netloc
        if host not in self.host_semaphores:
            self.host_semaphores[host] = Semaphore(self.max_per_host)
        return self.host_semaphores[host]
    
    def _should_crawl(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥çˆ¬å– URL"""
        # å»é‡
        if url in self.visited_urls:
            return False
        
        # è¿‡æ»¤æ— æ•ˆ URL
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False
        
        # åªçˆ¬å– HTTP/HTTPS
        if parsed.scheme not in ['http', 'https']:
            return False
        
        return True
    
    async def _extract_links(self, html: str, base_url: str) -> List[str]:
        """ä» HTML ä¸­æå–é“¾æ¥"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for tag in soup.find_all(['a', 'link']):
            href = tag.get('href')
            if href:
                absolute_url = urljoin(base_url, href)
                links.append(absolute_url)
        
        return links
    
    async def _fetch_with_retry(
        self,
        session: aiohttp.ClientSession,
        url: str
    ) -> CrawlResult:
        """å¸¦é‡è¯•çš„é¡µé¢è·å–"""
        for attempt in range(self.max_retries):
            try:
                start_time = time.time()
                
                async with session.get(
                    url,
                    timeout=aiohttp.ClientTimeout(total=self.timeout),
                    headers={'User-Agent': self.user_agent}
                ) as response:
                    html = await response.text()
                    
                    # æå–é¡µé¢ä¿¡æ¯
                    soup = BeautifulSoup(html, 'html.parser')
                    title = soup.title.string if soup.title else "No Title"
                    links = await self._extract_links(html, url)
                    
                    crawl_time = time.time() - start_time
                    
                    return CrawlResult(
                        url=url,
                        status_code=response.status,
                        title=title.strip() if title else "",
                        content_length=len(html),
                        links=links[:50],  # é™åˆ¶é“¾æ¥æ•°é‡
                        crawl_time=crawl_time
                    )
                    
            except asyncio.TimeoutError:
                error = f"Timeout (attempt {attempt + 1}/{self.max_retries})"
                logger.warning(f"â±ï¸ {url}: {error}")
                
            except aiohttp.ClientError as e:
                error = f"Client error: {e} (attempt {attempt + 1}/{self.max_retries})"
                logger.warning(f"âš ï¸ {url}: {error}")
                
            except Exception as e:
                error = f"Unexpected error: {e}"
                logger.error(f"âŒ {url}: {error}")
                break
            
            # æŒ‡æ•°é€€é¿
            if attempt < self.max_retries - 1:
                await asyncio.sleep(2 ** attempt)
        
        # æ‰€æœ‰é‡è¯•å¤±è´¥
        return CrawlResult(
            url=url,
            status_code=0,
            title="",
            content_length=0,
            links=[],
            crawl_time=0,
            error=error
        )
    
    async def crawl_url(
        self,
        session: aiohttp.ClientSession,
        url: str
    ) -> Optional[CrawlResult]:
        """çˆ¬å–å•ä¸ª URL"""
        if not self._should_crawl(url):
            return None
        
        # æ ‡è®°ä¸ºå·²è®¿é—®
        self.visited_urls.add(url)
        self.stats['total_requests'] += 1
        
        # è·å–ä¿¡å·é‡
        host_semaphore = self._get_host_semaphore(url)
        
        async with self.semaphore:  # å…¨å±€å¹¶å‘æ§åˆ¶
            async with host_semaphore:  # ä¸»æœºçº§å¹¶å‘æ§åˆ¶
                logger.info(f"ğŸ•·ï¸ çˆ¬å–: {url}")
                
                result = await self._fetch_with_retry(session, url)
                
                if result.error:
                    self.failed_urls.add(url)
                    self.stats['failed_requests'] += 1
                else:
                    self.stats['successful_requests'] += 1
                    self.stats['total_bytes'] += result.content_length
                
                self.results.append(result)
                return result
    
    async def crawl_recursive(
        self,
        start_url: str,
        max_depth: int = 2,
        max_pages: int = 100
    ) -> List[CrawlResult]:
        """é€’å½’çˆ¬å–"""
        self.stats['start_time'] = time.time()
        
        async with aiohttp.ClientSession() as session:
            # BFS é˜Ÿåˆ—
            queue = [(start_url, 0)]  # (url, depth)
            
            while queue and len(self.visited_urls) < max_pages:
                # æ‰¹é‡å¤„ç†å½“å‰å±‚çº§
                current_batch = []
                next_batch = []
                
                for url, depth in queue:
                    if depth <= max_depth and len(current_batch) < 10:
                        current_batch.append((url, depth))
                    elif depth < max_depth:
                        next_batch.append((url, depth))
                
                queue = next_batch
                
                # å¹¶å‘çˆ¬å–å½“å‰æ‰¹æ¬¡
                tasks = []
                for url, depth in current_batch:
                    task = asyncio.create_task(self.crawl_url(session, url))
                    tasks.append((task, depth))
                
                # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
                for task, depth in tasks:
                    result = await task
                    
                    # æ·»åŠ å­é“¾æ¥åˆ°é˜Ÿåˆ—
                    if result and not result.error and depth < max_depth:
                        for link in result.links[:10]:  # é™åˆ¶æ¯é¡µé“¾æ¥æ•°
                            if self._should_crawl(link):
                                queue.append((link, depth + 1))
        
        self.stats['end_time'] = time.time()
        return self.results
    
    def print_statistics(self):
        """æ‰“å°çˆ¬å–ç»Ÿè®¡"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è¯·æ±‚æ•°: {self.stats['total_requests']}")
        print(f"æˆåŠŸè¯·æ±‚: {self.stats['successful_requests']}")
        print(f"å¤±è´¥è¯·æ±‚: {self.stats['failed_requests']}")
        print(f"æ€»ä¸‹è½½é‡: {self.stats['total_bytes'] / 1024 / 1024:.2f} MB")
        print(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {self.stats['total_requests'] / duration:.2f} é¡µ/ç§’")
        print(f"å¹³å‡é¡µé¢å¤§å°: {self.stats['total_bytes'] / max(1, self.stats['successful_requests']) / 1024:.2f} KB")
        
        # å¤±è´¥ URL åˆ—è¡¨
        if self.failed_urls:
            print(f"\nâŒ å¤±è´¥çš„ URL ({len(self.failed_urls)} ä¸ª):")
            for url in list(self.failed_urls)[:5]:
                print(f"  - {url}")
    
    async def save_results(self, filename: str = "crawl_results.json"):
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        async with aiofiles.open(filename, 'w', encoding='utf-8') as f:
            data = {
                'stats': self.stats,
                'results': [asdict(r) for r in self.results]
            }
            await f.write(json.dumps(data, indent=2, ensure_ascii=False))
        
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

# çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°"""
    crawler = AsyncWebCrawler(
        max_concurrent=20,
        max_per_host=5,
        timeout=10
    )
    
    # å¼€å§‹çˆ¬å–
    start_url = "https://example.com"
    results = await crawler.crawl_recursive(
        start_url,
        max_depth=2,
        max_pages=50
    )
    
    # æ‰“å°ç»Ÿè®¡
    crawler.print_statistics()
    
    # ä¿å­˜ç»“æœ
    await crawler.save_results()
    
    # æ‰“å°å‰ 5 ä¸ªç»“æœ
    print("\nğŸ“ çˆ¬å–ç»“æœç¤ºä¾‹:")
    for result in results[:5]:
        print(f"\nURL: {result.url}")
        print(f"  æ ‡é¢˜: {result.title}")
        print(f"  çŠ¶æ€ç : {result.status_code}")
        print(f"  é¡µé¢å¤§å°: {result.content_length / 1024:.2f} KB")
        print(f"  çˆ¬å–æ—¶é—´: {result.crawl_time:.2f} ç§’")
        print(f"  é“¾æ¥æ•°: {len(result.links)}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 2. å¼‚æ­¥æ•°æ®åº“è¿æ¥æ± 

```python
import asyncio
import asyncpg
import aioredis
from typing import Any, Dict, List, Optional
from contextlib import asynccontextmanager
import json
import logging
from datetime import datetime
import time

logger = logging.getLogger(__name__)

class AsyncDatabasePool:
    """å¼‚æ­¥æ•°æ®åº“è¿æ¥æ± ç®¡ç†å™¨"""
    
    def __init__(
        self,
        postgres_dsn: str,
        redis_url: str,
        pg_min_size: int = 10,
        pg_max_size: int = 20,
        cache_ttl: int = 300
    ):
        self.postgres_dsn = postgres_dsn
        self.redis_url = redis_url
        self.pg_min_size = pg_min_size
        self.pg_max_size = pg_max_size
        self.cache_ttl = cache_ttl
        
        self.pg_pool: Optional[asyncpg.Pool] = None
        self.redis_pool: Optional[aioredis.Redis] = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'queries': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_query_time': 0
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        logger.info("ğŸ”§ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        
        # åˆ›å»º PostgreSQL è¿æ¥æ± 
        self.pg_pool = await asyncpg.create_pool(
            self.postgres_dsn,
            min_size=self.pg_min_size,
            max_size=self.pg_max_size,
            command_timeout=60
        )
        
        # åˆ›å»º Redis è¿æ¥æ± 
        self.redis_pool = await aioredis.create_redis_pool(
            self.redis_url,
            encoding='utf-8'
        )
        
        logger.info("âœ… æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.pg_pool:
            await self.pg_pool.close()
        
        if self.redis_pool:
            self.redis_pool.close()
            await self.redis_pool.wait_closed()
        
        logger.info("ğŸ”’ æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")
    
    @asynccontextmanager
    async def acquire_pg_connection(self):
        """è·å– PostgreSQL è¿æ¥"""
        async with self.pg_pool.acquire() as connection:
            yield connection
    
    def _cache_key(self, query: str, params: tuple) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        key_str = f"{query}:{str(params)}"
        return f"query:{hashlib.md5(key_str.encode()).hexdigest()}"
    
    async def execute_query(
        self,
        query: str,
        *params,
        use_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        self.stats['queries'] += 1
        start_time = time.time()
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cache_key = self._cache_key(query, params)
            cached = await self.redis_pool.get(cache_key)
            
            if cached:
                self.stats['cache_hits'] += 1
                logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {cache_key}")
                return json.loads(cached)
            else:
                self.stats['cache_misses'] += 1
        
        # æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
        async with self.acquire_pg_connection() as conn:
            rows = await conn.fetch(query, *params)
            result = [dict(row) for row in rows]
        
        # å†™å…¥ç¼“å­˜
        if use_cache and result:
            cache_key = self._cache_key(query, params)
            await self.redis_pool.setex(
                cache_key,
                self.cache_ttl,
                json.dumps(result, default=str)
            )
            logger.debug(f"ğŸ’¾ å†™å…¥ç¼“å­˜: {cache_key}")
        
        # æ›´æ–°ç»Ÿè®¡
        query_time = time.time() - start_time
        self.stats['total_query_time'] += query_time
        
        return result
    
    async def execute_transaction(self, operations: List[tuple]) -> bool:
        """æ‰§è¡Œäº‹åŠ¡"""
        async with self.acquire_pg_connection() as conn:
            async with conn.transaction():
                try:
                    for query, params in operations:
                        await conn.execute(query, *params)
                    return True
                except Exception as e:
                    logger.error(f"âŒ äº‹åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                    raise
    
    async def bulk_insert(
        self,
        table: str,
        records: List[Dict[str, Any]]
    ) -> int:
        """æ‰¹é‡æ’å…¥"""
        if not records:
            return 0
        
        # æ„å»ºæ’å…¥è¯­å¥
        columns = list(records[0].keys())
        values_template = ", ".join([f"${i+1}" for i in range(len(columns))])
        
        query = f"""
            INSERT INTO {table} ({", ".join(columns)})
            VALUES ({values_template})
        """
        
        # æ‰¹é‡æ‰§è¡Œ
        async with self.acquire_pg_connection() as conn:
            count = 0
            async with conn.transaction():
                for record in records:
                    values = [record[col] for col in columns]
                    await conn.execute(query, *values)
                    count += 1
        
        # æ¸…é™¤ç›¸å…³ç¼“å­˜
        await self.invalidate_cache_pattern(f"query:*{table}*")
        
        return count
    
    async def invalidate_cache_pattern(self, pattern: str):
        """æŒ‰æ¨¡å¼æ¸…é™¤ç¼“å­˜"""
        cursor = b'0'
        while cursor:
            cursor, keys = await self.redis_pool.scan(
                cursor, match=pattern
            )
            if keys:
                await self.redis_pool.delete(*keys)
                logger.debug(f"ğŸ—‘ï¸ æ¸…é™¤ç¼“å­˜: {len(keys)} ä¸ªé”®")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.stats.copy()
        
        if stats['queries'] > 0:
            stats['cache_hit_rate'] = stats['cache_hits'] / stats['queries']
            stats['avg_query_time'] = stats['total_query_time'] / stats['queries']
        
        return stats

# è¿æ¥æ± ä½¿ç”¨ç¤ºä¾‹
class UserRepository:
    """ç”¨æˆ·ä»“åº“ï¼ˆä½¿ç”¨è¿æ¥æ± ï¼‰"""
    
    def __init__(self, db_pool: AsyncDatabasePool):
        self.db = db_pool
    
    async def create_table(self):
        """åˆ›å»ºç”¨æˆ·è¡¨"""
        query = """
            CREATE TABLE IF NOT EXISTS users (
                id SERIAL PRIMARY KEY,
                username VARCHAR(100) UNIQUE NOT NULL,
                email VARCHAR(255) UNIQUE NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """
        async with self.db.acquire_pg_connection() as conn:
            await conn.execute(query)
    
    async def find_by_id(self, user_id: int) -> Optional[Dict]:
        """æ ¹æ® ID æŸ¥æ‰¾ç”¨æˆ·"""
        query = "SELECT * FROM users WHERE id = $1"
        results = await self.db.execute_query(query, user_id)
        return results[0] if results else None
    
    async def find_by_username(self, username: str) -> Optional[Dict]:
        """æ ¹æ®ç”¨æˆ·åæŸ¥æ‰¾"""
        query = "SELECT * FROM users WHERE username = $1"
        results = await self.db.execute_query(query, username)
        return results[0] if results else None
    
    async def create_user(self, username: str, email: str) -> Dict:
        """åˆ›å»ºç”¨æˆ·"""
        query = """
            INSERT INTO users (username, email)
            VALUES ($1, $2)
            RETURNING *
        """
        
        async with self.db.acquire_pg_connection() as conn:
            row = await conn.fetchrow(query, username, email)
            user = dict(row)
        
        # æ¸…é™¤ç¼“å­˜
        await self.db.invalidate_cache_pattern("query:*users*")
        
        return user
    
    async def update_user(self, user_id: int, **kwargs) -> Optional[Dict]:
        """æ›´æ–°ç”¨æˆ·"""
        if not kwargs:
            return None
        
        # æ„å»ºæ›´æ–°è¯­å¥
        set_clauses = []
        values = []
        for i, (key, value) in enumerate(kwargs.items(), 1):
            set_clauses.append(f"{key} = ${i}")
            values.append(value)
        
        values.append(user_id)
        query = f"""
            UPDATE users
            SET {", ".join(set_clauses)}, updated_at = CURRENT_TIMESTAMP
            WHERE id = ${len(values)}
            RETURNING *
        """
        
        async with self.db.acquire_pg_connection() as conn:
            row = await conn.fetchrow(query, *values)
            
        if row:
            # æ¸…é™¤ç¼“å­˜
            await self.db.invalidate_cache_pattern(f"query:*users*")
            return dict(row)
        
        return None

async def database_demo():
    """æ•°æ®åº“è¿æ¥æ± æ¼”ç¤º"""
    # åˆå§‹åŒ–è¿æ¥æ± 
    db_pool = AsyncDatabasePool(
        postgres_dsn="postgresql://user:pass@localhost/testdb",
        redis_url="redis://localhost:6379",
        pg_min_size=5,
        pg_max_size=10
    )
    
    try:
        await db_pool.initialize()
        
        # åˆ›å»ºç”¨æˆ·ä»“åº“
        user_repo = UserRepository(db_pool)
        await user_repo.create_table()
        
        # å¹¶å‘æµ‹è¯•
        async def create_and_query_user(index: int):
            """åˆ›å»ºå¹¶æŸ¥è¯¢ç”¨æˆ·"""
            # åˆ›å»ºç”¨æˆ·
            user = await user_repo.create_user(
                username=f"user_{index}",
                email=f"user_{index}@example.com"
            )
            logger.info(f"âœ… åˆ›å»ºç”¨æˆ·: {user['username']}")
            
            # æŸ¥è¯¢ç”¨æˆ·ï¼ˆç¬¬ä¸€æ¬¡ï¼Œç¼“å­˜æœªå‘½ä¸­ï¼‰
            found = await user_repo.find_by_id(user['id'])
            
            # å†æ¬¡æŸ¥è¯¢ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
            found_cached = await user_repo.find_by_id(user['id'])
            
            return found
        
        # å¹¶å‘åˆ›å»ºå’ŒæŸ¥è¯¢ç”¨æˆ·
        tasks = []
        for i in range(10):
            task = asyncio.create_task(create_and_query_user(i))
            tasks.append(task)
        
        users = await asyncio.gather(*tasks)
        
        # æ‰“å°ç»Ÿè®¡
        stats = db_pool.get_statistics()
        print("\nğŸ“Š æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»æŸ¥è¯¢æ•°: {stats['queries']}")
        print(f"  ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
        print(f"  ç¼“å­˜æœªå‘½ä¸­: {stats['cache_misses']}")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {stats.get('cache_hit_rate', 0):.2%}")
        print(f"  å¹³å‡æŸ¥è¯¢æ—¶é—´: {stats.get('avg_query_time', 0):.3f} ç§’")
        
    finally:
        await db_pool.close()

if __name__ == "__main__":
    asyncio.run(database_demo())
```

## ğŸ” æ·±åº¦è§£æ

### 1. å¼‚æ­¥ vs å¤šçº¿ç¨‹ vs å¤šè¿›ç¨‹

```python
import asyncio
import threading
import multiprocessing
import time
import concurrent.futures
from typing import List, Callable
import requests
import aiohttp

class ConcurrencyComparison:
    """å¹¶å‘æ¨¡å‹å¯¹æ¯”"""
    
    @staticmethod
    def cpu_bound_task(n: int) -> int:
        """CPU å¯†é›†å‹ä»»åŠ¡ï¼šè®¡ç®—æ–æ³¢é‚£å¥‘æ•°"""
        if n <= 1:
            return n
        return ConcurrencyComparison.cpu_bound_task(n-1) + \
               ConcurrencyComparison.cpu_bound_task(n-2)
    
    @staticmethod
    def io_bound_task(url: str) -> int:
        """I/O å¯†é›†å‹ä»»åŠ¡ï¼šç½‘ç»œè¯·æ±‚"""
        response = requests.get(url)
        return len(response.content)
    
    @staticmethod
    async def async_io_bound_task(session: aiohttp.ClientSession, url: str) -> int:
        """å¼‚æ­¥ I/O ä»»åŠ¡"""
        async with session.get(url) as response:
            content = await response.read()
            return len(content)
    
    @staticmethod
    def test_sequential(task_func: Callable, args_list: List) -> float:
        """é¡ºåºæ‰§è¡Œæµ‹è¯•"""
        start = time.time()
        results = []
        
        for args in args_list:
            result = task_func(args)
            results.append(result)
        
        return time.time() - start
    
    @staticmethod
    def test_threading(task_func: Callable, args_list: List) -> float:
        """å¤šçº¿ç¨‹æµ‹è¯•"""
        start = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(task_func, args) for args in args_list]
            results = [f.result() for f in concurrent.futures.as_completed(futures)]
        
        return time.time() - start
    
    @staticmethod
    def test_multiprocessing(task_func: Callable, args_list: List) -> float:
        """å¤šè¿›ç¨‹æµ‹è¯•"""
        start = time.time()
        
        with multiprocessing.Pool(processes=4) as pool:
            results = pool.map(task_func, args_list)
        
        return time.time() - start
    
    @staticmethod
    async def test_asyncio(urls: List[str]) -> float:
        """å¼‚æ­¥æµ‹è¯•"""
        start = time.time()
        
        async with aiohttp.ClientSession() as session:
            tasks = [
                ConcurrencyComparison.async_io_bound_task(session, url)
                for url in urls
            ]
            results = await asyncio.gather(*tasks)
        
        return time.time() - start

async def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("ğŸ”¬ å¹¶å‘æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("=" * 60)
    
    # CPU å¯†é›†å‹ä»»åŠ¡æµ‹è¯•
    print("\nğŸ“Š CPU å¯†é›†å‹ä»»åŠ¡ï¼ˆè®¡ç®—æ–æ³¢é‚£å¥‘æ•°ï¼‰:")
    cpu_tasks = [30] * 10
    
    seq_time = ConcurrencyComparison.test_sequential(
        ConcurrencyComparison.cpu_bound_task, cpu_tasks
    )
    print(f"  é¡ºåºæ‰§è¡Œ: {seq_time:.2f} ç§’")
    
    thread_time = ConcurrencyComparison.test_threading(
        ConcurrencyComparison.cpu_bound_task, cpu_tasks
    )
    print(f"  å¤šçº¿ç¨‹: {thread_time:.2f} ç§’")
    
    mp_time = ConcurrencyComparison.test_multiprocessing(
        ConcurrencyComparison.cpu_bound_task, cpu_tasks
    )
    print(f"  å¤šè¿›ç¨‹: {mp_time:.2f} ç§’")
    
    # I/O å¯†é›†å‹ä»»åŠ¡æµ‹è¯•
    print("\nğŸ“Š I/O å¯†é›†å‹ä»»åŠ¡ï¼ˆç½‘ç»œè¯·æ±‚ï¼‰:")
    urls = ["https://httpbin.org/delay/1"] * 10
    
    seq_io_time = ConcurrencyComparison.test_sequential(
        ConcurrencyComparison.io_bound_task, urls
    )
    print(f"  é¡ºåºæ‰§è¡Œ: {seq_io_time:.2f} ç§’")
    
    thread_io_time = ConcurrencyComparison.test_threading(
        ConcurrencyComparison.io_bound_task, urls
    )
    print(f"  å¤šçº¿ç¨‹: {thread_io_time:.2f} ç§’")
    
    async_io_time = await ConcurrencyComparison.test_asyncio(urls)
    print(f"  å¼‚æ­¥: {async_io_time:.2f} ç§’")
    
    # ç»“è®º
    print("\nğŸ“ˆ ç»“è®º:")
    print("  - CPU å¯†é›†å‹ï¼šå¤šè¿›ç¨‹ > å¤šçº¿ç¨‹ > é¡ºåºæ‰§è¡Œ")
    print("  - I/O å¯†é›†å‹ï¼šå¼‚æ­¥ > å¤šçº¿ç¨‹ > é¡ºåºæ‰§è¡Œ")
    print("  - å¼‚æ­¥é€‚åˆ I/O å¯†é›†å‹ï¼Œå¤šè¿›ç¨‹é€‚åˆ CPU å¯†é›†å‹")

if __name__ == "__main__":
    asyncio.run(performance_comparison())
```

## âš ï¸ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜1ï¼šåç¨‹æœªæ­£ç¡®ç­‰å¾…
**ç—‡çŠ¶**: `RuntimeWarning: coroutine was never awaited`
**åŸå› **: è°ƒç”¨åç¨‹å‡½æ•°æ—¶å¿˜è®°ä½¿ç”¨ await
**è§£å†³**:
```python
# âŒ é”™è¯¯
async def main():
    fetch_data()  # å¿˜è®° await

# âœ… æ­£ç¡®
async def main():
    await fetch_data()
```

### é—®é¢˜2ï¼šäº‹ä»¶å¾ªç¯å†²çª
**ç—‡çŠ¶**: `RuntimeError: This event loop is already running`
**åŸå› **: åœ¨å·²è¿è¡Œçš„äº‹ä»¶å¾ªç¯ä¸­å°è¯•è¿è¡Œå¦ä¸€ä¸ª
**è§£å†³**:
```python
# ä½¿ç”¨ nest_asyncio è§£å†³ Jupyter ç¯å¢ƒé—®é¢˜
import nest_asyncio
nest_asyncio.apply()
```

### é—®é¢˜3ï¼šå†…å­˜æ³„æ¼
**ç—‡çŠ¶**: é•¿æ—¶é—´è¿è¡Œåå†…å­˜æŒç»­å¢é•¿
**åŸå› **: ä»»åŠ¡æœªæ­£ç¡®æ¸…ç†æˆ–å¾ªç¯å¼•ç”¨
**è§£å†³**:
```python
# ä½¿ç”¨å¼±å¼•ç”¨é¿å…å¾ªç¯å¼•ç”¨
import weakref

class TaskManager:
    def __init__(self):
        self.tasks = weakref.WeakSet()
    
    def add_task(self, task):
        self.tasks.add(task)
```

## ğŸš€ è¿›é˜¶æ‰©å±•

### 1. å¼‚æ­¥ç”Ÿæˆå™¨å’Œå¼‚æ­¥è¿­ä»£å™¨
### 2. å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
### 3. å¼‚æ­¥é˜Ÿåˆ—å’Œç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼
### 4. ä¸åŒæ­¥ä»£ç çš„äº’æ“ä½œ
### 5. å¼‚æ­¥æµ‹è¯•æœ€ä½³å®è·µ

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **åˆç†æ§åˆ¶å¹¶å‘æ•°**ï¼šä½¿ç”¨ Semaphore é™åˆ¶å¹¶å‘
2. **è¿æ¥æ± å¤ç”¨**ï¼šé¿å…é¢‘ç¹åˆ›å»ºé”€æ¯è¿æ¥
3. **æ‰¹é‡æ“ä½œ**ï¼šå‡å°‘ I/O æ¬¡æ•°
4. **ç¼“å­˜ç­–ç•¥**ï¼šåˆç†ä½¿ç”¨å†…å­˜å’Œ Redis ç¼“å­˜
5. **ç›‘æ§å’Œè°ƒè¯•**ï¼šä½¿ç”¨ aiomonitor ç›‘æ§è¿è¡ŒçŠ¶æ€

## ğŸ”— å‚è€ƒèµ„æº

- [Python asyncio å®˜æ–¹æ–‡æ¡£](https://docs.python.org/3/library/asyncio.html)
- [aiohttp æ–‡æ¡£](https://docs.aiohttp.org/)
- [asyncpg æ–‡æ¡£](https://magicstack.github.io/asyncpg/)
- [Python Async/Await æ•™ç¨‹](https://realpython.com/async-io-python/)

## ğŸ’­ æ€è€ƒé¢˜

1. ä»€ä¹ˆæƒ…å†µä¸‹å¼‚æ­¥ç¼–ç¨‹åè€Œä¼šé™ä½æ€§èƒ½ï¼Ÿ
2. å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æŒèƒŒå‹çš„å¼‚æ­¥æµå¤„ç†ç³»ç»Ÿï¼Ÿ
3. åœ¨å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œå¦‚ä½•å®ç°å¼‚æ­¥æœåŠ¡é—´é€šä¿¡ï¼Ÿ

## ğŸ“ æ€»ç»“

Python å¼‚æ­¥ç¼–ç¨‹æ˜¯å¤„ç†é«˜å¹¶å‘ I/O å¯†é›†å‹ä»»åŠ¡çš„åˆ©å™¨ã€‚é€šè¿‡æ·±å…¥ç†è§£äº‹ä»¶å¾ªç¯æœºåˆ¶ã€æŒæ¡åç¨‹ç¼–ç¨‹æ¨¡å¼ã€åˆç†è®¾è®¡å¹¶å‘æ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºå‡ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„å¼‚æ­¥åº”ç”¨ã€‚è®°ä½ï¼Œå¼‚æ­¥ç¼–ç¨‹ä¸æ˜¯é“¶å¼¹ï¼Œè¦æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„å¹¶å‘æ¨¡å‹ã€‚

---

**ç›¸å…³æ–‡ç« **: [æ–‡æ¡£ä¸­å¿ƒ](../../../docs/) - æŸ¥çœ‹æ›´å¤šæŠ€æœ¯æ–‡æ¡£å’Œå­¦ä¹ èµ„æ–™